# AWS Strands Nova Voice Assistant - Sequence Flow Diagram

## System Architecture Overview

The system consists of:
- **Frontend**: React app with voice interface
- **Backend**: Python WebSocket server with AWS Strands agents
- **AWS Services**: Bedrock (Nova Sonic), EC2, SSM, Backup

## Sequence Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant Frontend as React Frontend<br/>(VoiceAgent.js)
    participant WS as WebSocket Server<br/>(server.py)
    participant S2S as S2S Session Manager<br/>(s2s_session_manager.py)
    participant Bedrock as Amazon Nova Sonic<br/>(Bedrock)
    participant Supervisor as Supervisor Agent<br/>(supervisor_agent.py)
    participant Orchestrator as Agent Orchestrator<br/>(orchestrator.py)
    participant SpecAgent as Specialized Agent<br/>(EC2/SSM/Backup)
    participant AWS as AWS Services<br/>(EC2/SSM/Backup APIs)

    Note over User, AWS: 1. Session Initialization
    User->>Frontend: Click "Start Conversation"
    Frontend->>WS: WebSocket Connection (ws://localhost:8080)
    WS->>S2S: Initialize S2sSessionManager
    S2S->>Bedrock: Create bidirectional stream
    Bedrock-->>S2S: Stream established
    S2S-->>WS: Stream ready
    WS-->>Frontend: Connection confirmed

    Note over User, AWS: 2. Session Setup Events (Initialization Sequence)
    
    Note right of Frontend: Step 2a: Start Session
    Frontend->>WS: sessionStart event
    Note right of Frontend: JSON: {"event": {"sessionStart": {"inferenceConfiguration": {...}}}}
    WS->>S2S: Forward sessionStart
    S2S->>Bedrock: Send sessionStart to Nova Sonic
    
    Note right of Frontend: Step 2b: Configure Prompt with Tools
    Frontend->>WS: promptStart event (with supervisorAgent tool)
    Note right of Frontend: Includes tool definition for AWS agent routing
    WS->>S2S: Forward promptStart
    S2S->>Bedrock: Send promptStart with supervisorAgent tool config
    
    Note right of Frontend: Step 2c: Send System Instructions
    Frontend->>WS: contentStartText event
    Frontend->>WS: textInput event (system prompt)
    Frontend->>WS: contentEnd event
    Note right of Frontend: System prompt: "You are AWS assistant, use supervisorAgent tool"
    WS->>S2S: Forward system prompt sequence
    S2S->>Bedrock: Send system instructions to Nova Sonic
    
    Note right of Frontend: Step 2d: Enable Audio Input
    Frontend->>WS: contentStartAudio event
    Note right of Frontend: Configures 16kHz audio input for voice
    WS->>S2S: Forward contentStartAudio
    S2S->>Bedrock: Enable audio input stream

    Note over User, AWS: 3. Voice Input Processing
    User->>Frontend: Speak into microphone
    Frontend->>Frontend: Capture audio via MediaRecorder
    Frontend->>Frontend: Resample to 16kHz, convert to base64
    Frontend->>WS: audioInput event (base64 audio chunks)
    WS->>S2S: Add to audio queue
    S2S->>S2S: Process audio queue
    S2S->>Bedrock: Send audioInput events

    Note over User, AWS: 4. Speech-to-Text & Tool Processing
    Bedrock->>S2S: textOutput event (transcribed speech)
    S2S->>WS: Forward textOutput
    WS->>Frontend: Send textOutput
    Frontend->>Frontend: Display user message in chat
    
    Note right of Bedrock: Nova Sonic decides to use supervisorAgent tool
    Bedrock->>S2S: toolUse event (supervisorAgent call)
    Note right of S2S: Store: toolName="supervisorAgent", toolUseId, content=user_query
    S2S->>S2S: Store tool use details in memory
    
    Bedrock->>S2S: contentEnd event (TOOL type)
    Note right of S2S: Trigger: Process the stored tool use
    S2S->>S2S: Call processToolUse() function

    Note over User, AWS: 5. Agent Processing Chain
    S2S->>Supervisor: Call supervisor_agent.query(user_query)
    Supervisor->>Supervisor: Analyze query keywords
    Supervisor->>Orchestrator: Route to appropriate agent
    Orchestrator->>SpecAgent: Forward query to specialized agent
    SpecAgent->>AWS: Make AWS API calls
    AWS-->>SpecAgent: Return AWS data
    SpecAgent-->>Orchestrator: Return formatted response
    Orchestrator-->>Supervisor: Return response
    Supervisor-->>S2S: Return final response

    Note over User, AWS: 6. Tool Result Processing
    S2S->>S2S: Create tool result events
    S2S->>Bedrock: Send contentStartTool
    S2S->>Bedrock: Send toolResult with agent response
    S2S->>Bedrock: Send contentEnd

    Note over User, AWS: 7. Text-to-Speech Response
    Bedrock->>S2S: textOutput event (assistant response)
    S2S->>WS: Forward textOutput
    WS->>Frontend: Send textOutput
    Frontend->>Frontend: Display assistant message in chat
    
    Bedrock->>S2S: audioOutput events (base64 audio chunks)
    S2S->>WS: Forward audioOutput
    WS->>Frontend: Send audioOutput
    Frontend->>Frontend: Convert base64 to Float32Array
    Frontend->>Frontend: Play audio via AudioPlayer
    Frontend->>User: Speak response

    Note over User, AWS: 8. Session Cleanup (when user ends conversation)
    User->>Frontend: Click "End Conversation"
    Frontend->>WS: contentEnd + promptEnd + sessionEnd
    WS->>S2S: Forward end events
    S2S->>Bedrock: Send end events
    S2S->>S2S: Close stream
    Frontend->>Frontend: Stop microphone, cleanup
```

## Detailed Session Setup Explanation

### What Happens When User Clicks "Start Conversation"

The session setup is a **4-step initialization sequence** that prepares Nova Sonic for voice interaction:

#### Step 2a: sessionStart Event
**Frontend Code (VoiceAgent.js):**
```javascript
// When WebSocket opens
this.sendEvent(S2sEvent.sessionStart());
```

**Event Structure:**
```json
{
  "event": {
    "sessionStart": {
      "inferenceConfiguration": {
        "maxTokens": 1024,
        "topP": 1.0,
        "temperature": 1.0
      }
    }
  }
}
```

**Purpose:** Tells Nova Sonic to start a new conversation session with AI model parameters.

#### Step 2b: promptStart Event (Tool Configuration)
**Frontend Code (VoiceAgent.js):**
```javascript
// Configure tool for AWS agent routing
const toolConfig = {
  "tools": [{
    "toolSpec": {
      "name": "supervisorAgent",
      "description": "Routes queries to specialized agents for EC2, Backup, and SSM",
      "inputSchema": {
        "json": JSON.stringify({
          "type": "object",
          "properties": {
            "query": {"type": "string", "description": "The user query about AWS services"}
          },
          "required": ["query"]
        })
      }
    }
  }]
};

this.sendEvent(S2sEvent.promptStart(promptName, audioConfig, toolConfig));
```

**Purpose:** Configures Nova Sonic to use the `supervisorAgent` tool when it needs to handle AWS queries.

#### Step 2c: System Prompt Setup
**Frontend Code (VoiceAgent.js):**
```javascript
// Send system instructions
this.sendEvent(S2sEvent.contentStartText(promptName, textContentName));
this.sendEvent(S2sEvent.textInput(promptName, textContentName, this.state.configSystemPrompt));
this.sendEvent(S2sEvent.contentEnd(promptName, textContentName));
```

**System Prompt Content:**
```
"You are a specialized AWS assistant that ONLY helps with AWS services and operations.
When you need to get information about AWS services, use the supervisorAgent tool.
Keep responses concise and focused on AWS services only."
```

**Purpose:** Gives Nova Sonic its personality and instructions on when to use the AWS tool.

#### Step 2d: Enable Audio Input
**Frontend Code (VoiceAgent.js):**
```javascript
this.sendEvent(S2sEvent.contentStartAudio(promptName, audioContentName));
```

**Audio Configuration:**
```json
{
  "mediaType": "audio/lpcm",
  "sampleRateHertz": 16000,
  "sampleSizeBits": 16,
  "channelCount": 1,
  "audioType": "SPEECH",
  "encoding": "base64"
}
```

**Purpose:** Tells Nova Sonic to expect incoming voice audio in 16kHz format.

### Why This Sequence Matters

1. **sessionStart**: Initializes the AI conversation engine
2. **promptStart**: Defines what tools (AWS agents) are available
3. **System Prompt**: Gives the AI its instructions and personality
4. **Audio Setup**: Enables voice input/output capabilities

After these 4 steps, Nova Sonic is ready to:
- Listen to voice input
- Understand it's an AWS assistant
- Know it can call the supervisorAgent tool for AWS operations
- Respond with both text and voice

## Detailed Tool Use Process Explanation

### What Happens When Nova Sonic Decides to Use a Tool

The tool use process is a **3-step sequence** where Nova Sonic calls your AWS agents:

#### Step 1: toolUse Event (Nova Sonic's Decision)
**When:** After Nova Sonic transcribes speech and decides it needs to call the supervisorAgent tool

**Backend Code (s2s_session_manager.py):**
```python
# Handle tool use detection
if event_name == 'toolUse':
    self.toolUseContent = json_data['event']['toolUse']
    self.toolName = json_data['event']['toolUse']['toolName']
    self.toolUseId = json_data['event']['toolUse']['toolUseId']
    debug_print(f"Tool use detected: {self.toolName}, ID: {self.toolUseId}")
```

**Example toolUse Event from Bedrock:**
```json
{
  "event": {
    "toolUse": {
      "toolName": "supervisorAgent",
      "toolUseId": "12345-abcde-67890",
      "content": "{\"query\": \"list my EC2 instances\"}"
    }
  }
}
```

**What's Stored:**
- `toolName` = "supervisorAgent"
- `toolUseId` = unique identifier for this tool call
- `content` = the user's query that needs to be processed

#### Step 2: Store Tool Use Details
**Purpose:** The S2S manager temporarily stores the tool information because Bedrock sends tool data in chunks.

**What Gets Stored:**
```python
self.toolUseContent = {
    "toolName": "supervisorAgent",
    "toolUseId": "12345-abcde-67890", 
    "content": "{\"query\": \"list my EC2 instances\"}"
}
self.toolName = "supervisorAgent"
self.toolUseId = "12345-abcde-67890"
```

#### Step 3: contentEnd Event Triggers Processing
**When:** Bedrock sends a contentEnd event with type="TOOL", signaling the tool call is complete

**Backend Code (s2s_session_manager.py):**
```python
# Process tool use when content ends
elif event_name == 'contentEnd' and json_data['event'][event_name].get('type') == 'TOOL':
    prompt_name = json_data['event']['contentEnd'].get("promptName")
    debug_print("Processing tool use and sending result")
    
    # THIS IS WHERE THE MAGIC HAPPENS
    toolResult = await self.processToolUse(self.toolName, self.toolUseContent)
```

#### Step 4: processToolUse Function (The Core Logic)
**Backend Code (s2s_session_manager.py):**
```python
async def processToolUse(self, toolName, toolUseContent):
    """Process tool use with Supervisor Agent"""
    print(f"Tool Use Content: {toolUseContent}")
    
    # Extract the user's query
    content = toolUseContent.get("content")
    
    if toolName == "supervisoragent":
        # Parse the query from JSON
        if isinstance(content, str):
            try:
                content_obj = json.loads(content)
                query = content_obj.get("query", content)
            except:
                query = content
        
        # Call the supervisor agent (THIS ROUTES TO AWS AGENTS)
        result = await self.supervisor_agent.query(query)
        
        # Limit response for voice (800 chars max)
        if len(result) > 800:
            result = result[:800] + "... (truncated for voice)"
        
        return {"result": result}
```

#### Step 5: Send Tool Result Back to Bedrock
**Backend Code (s2s_session_manager.py):**
```python
# Send tool start event
toolContent = str(uuid.uuid4())
tool_start_event = S2sEvent.content_start_tool(prompt_name, toolContent, self.toolUseId)
await self.send_raw_event(tool_start_event)

# Send tool result event  
tool_result_event = S2sEvent.text_input_tool(prompt_name, toolContent, result)
await self.send_raw_event(tool_result_event)

# Send tool content end event
tool_content_end_event = S2sEvent.content_end(prompt_name, toolContent)
await self.send_raw_event(tool_content_end_event)
```

### The Complete Flow in Simple Terms

1. **User says:** "List my EC2 instances"
2. **Nova Sonic thinks:** "This is an AWS question, I need to use supervisorAgent tool"
3. **Nova Sonic sends:** toolUse event with the query
4. **S2S Manager stores:** Tool name, ID, and query content
5. **Nova Sonic sends:** contentEnd to signal "tool call complete"
6. **S2S Manager processes:** Calls processToolUse() function
7. **processToolUse() calls:** supervisor_agent.query("list my EC2 instances")
8. **Supervisor routes to:** EC2Agent which calls AWS APIs
9. **Result flows back:** EC2 data → Supervisor → processToolUse → Nova Sonic
10. **Nova Sonic responds:** With both text and voice containing EC2 instance information

### Why This 3-Step Process?

**Bedrock's Streaming Nature:** Nova Sonic sends data in chunks, so the S2S manager needs to:
1. **Collect** all the tool information as it arrives
2. **Wait** for the signal that the tool call is complete  
3. **Process** the complete tool call and send results back

This ensures the tool call is complete before trying to execute it!

## Key Data Flow Functions

### Frontend to Backend Communication

**Function: `sendEvent(event)` in VoiceAgent.js**
```javascript
sendEvent(event) {
    if (this.socket && this.socket.readyState === WebSocket.OPEN) {
        this.socket.send(JSON.stringify(event));
        // Event logging and display
    }
}
```

**Audio Processing Function: `startMicrophone()` in VoiceAgent.js**
```javascript
processor.onaudioprocess = async (e) => {
    // Resample audio to 16kHz
    // Convert to Int16Array then base64
    const base64Data = btoa(String.fromCharCode(...new Uint8Array(pcmData.buffer)));
    // Send via WebSocket
    this.sendEvent(S2sEvent.audioInput(promptName, audioContentName, base64Data));
}
```

### Backend Data Reception

**Function: `websocket_handler()` in server.py**
```python
async def websocket_handler(websocket, path, config):
    async for message in websocket:
        data = json.loads(message)
        if 'event' in data:
            event_type = list(data['event'].keys())[0]
            if event_type == 'audioInput':
                # Extract and queue audio data
                stream_manager.add_audio_chunk(prompt_name, content_name, audio_base64)
            else:
                # Forward other events to Bedrock
                await stream_manager.send_raw_event(data)
```

**Function: `add_audio_chunk()` in s2s_session_manager.py**
```python
def add_audio_chunk(self, prompt_name, content_name, audio_data):
    self.audio_input_queue.put_nowait({
        'prompt_name': prompt_name,
        'content_name': content_name,
        'audio_bytes': audio_data  # base64 string
    })
```

### Agent Processing

**Function: `processToolUse()` in s2s_session_manager.py**
```python
async def processToolUse(self, toolName, toolUseContent):
    if toolName == "supervisoragent":
        # Extract query from tool content
        query = content_obj.get("query", content)
        # Call supervisor agent
        result = await self.supervisor_agent.query(query)
        # Limit response length for voice
        if len(result) > 800:
            result = result[:800] + "... (truncated for voice)"
        return {"result": result}
```

**Function: `route_query()` in supervisor_agent.py**
```python
async def route_query(self, query: str) -> str:
    # Determine which specialized agent to use
    agent_name = self._determine_agent(query)
    specialized_agent = self.specialized_agents[agent_name]
    # Route to specialized agent
    response = specialized_agent(query)
    return response
```

### Backend to Frontend Communication

**Function: `forward_responses()` in server.py**
```python
async def forward_responses(websocket, stream_manager):
    while stream_manager.is_active:
        # Get response from Bedrock
        response = await stream_manager.output_queue.get()
        # Send to WebSocket
        event = json.dumps(response)
        await websocket.send(event)
```

**Function: `handleIncomingMessage()` in VoiceAgent.js**
```javascript
handleIncomingMessage(message) {
    const eventType = Object.keys(message?.event)[0];
    switch(eventType) {
        case "textOutput":
            // Update chat messages
            chatMessages[contentId].content = content;
            break;
        case "audioOutput":
            // Convert base64 to audio and play
            const audioData = base64ToFloat32Array(base64Data);
            this.audioPlayer.playAudio(audioData);
            break;
    }
}
```

## Data Flow Summary

1. **Voice Input**: User speaks → Frontend captures audio → Resamples to 16kHz → Converts to base64 → Sends via WebSocket
2. **Backend Processing**: WebSocket server receives → S2S manager queues audio → Sends to Bedrock Nova Sonic
3. **Speech-to-Text**: Bedrock transcribes speech → Returns textOutput events
4. **Tool Execution**: Bedrock calls supervisorAgent tool → S2S manager processes → Routes through agent hierarchy
5. **AWS Operations**: Specialized agents make AWS API calls → Return structured responses
6. **Text-to-Speech**: Bedrock converts response to speech → Returns audioOutput events
7. **Voice Output**: Frontend receives base64 audio → Converts to Float32Array → Plays through speakers

The system maintains real-time bidirectional communication using WebSockets and AWS Bedrock's streaming capabilities, with sophisticated audio processing and multi-agent routing for AWS service operations.
